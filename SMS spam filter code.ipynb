{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data mining assignment 1 question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6\t[45 points] Coding for Data Mining Beginners\n",
    "\n",
    "#In this coding exercise, any programming languages should be okay to use, but you are suggested to use Python or MATLAB for this coding practice. You are required to submit your cod-ing file which is executable. For Python, you can use Jupyter Notebook (see https://www. dataquest.io/blog/jupyter-notebook-tutorial if you are a beginner). For ex-ample, you can create an IPython notebook file ending with the .ipynb extension. Please contact the TA if you need help with it. The basic data loading and primitive pre-processing API will be provided using sklearn Python package.\n",
    "\n",
    "#You are given the SMS Spam Collection data set SMS.txt attached with this homework. In the file, each line is an SMS text message followed by a word indicating whether the message is a spam message (“spam”) or a normal message (“ham”). There are 5574 messages in total.\n",
    "\n",
    "#[10 points] Read the data set and output the number of spam messages. For example, the output of this part can be in the format of “There are ?? spam messages in the data set.”. We assume the data file is stored in the same directory as your coding file.\n",
    "\n",
    "#[10 points] Output the top 5 most frequent words in the whole dataset.\n",
    "\n",
    "#[10 points] Remove all the stop words in the dataset, which are given in the attached file stopwords.txt. Then output the top 5 most frequent words in the whole dataset.\n",
    "\n",
    "#[15 points] In this part, use the dataset with the stopwords removed. Convert each mes-sage item into a numerical vector using bag of words. To be more specific, each message would be converted into a vector where each entry of the vector represents the count of the corresponding word in that message. In python, you can use the CountVectorizer function in the sklearn package to vectorize the messages. Please calculate and output the cosine similarity between the 1st and the 50th messages in the data set. The output cosine similarity should be rounded to 4 decimal places. You should implement the details for calculating the cosine similarity from scratch. (Note: in Python, indices start from 0 instead of 1.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries and packages\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>go until jurong point crazy available only in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>free entry in a wkly comp to win fa cup final ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>u dun say so early hor u c already then say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>nah i don t think he goes to usf he lives arou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>spam</td>\n",
       "      <td>this is the nd time we have tried contact u u ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>will ¨¹ b going to esplanade fr home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>pity was in mood for that so any other suggest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5572</th>\n",
       "      <td>ham</td>\n",
       "      <td>the guy did some bitching but i acted like i d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5573</th>\n",
       "      <td>ham</td>\n",
       "      <td>rofl its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5574 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         A                                                  B\n",
       "0      ham  go until jurong point crazy available only in ...\n",
       "1      ham                            ok lar joking wif u oni\n",
       "2     spam  free entry in a wkly comp to win fa cup final ...\n",
       "3      ham        u dun say so early hor u c already then say\n",
       "4      ham  nah i don t think he goes to usf he lives arou...\n",
       "...    ...                                                ...\n",
       "5569  spam  this is the nd time we have tried contact u u ...\n",
       "5570   ham               will ¨¹ b going to esplanade fr home\n",
       "5571   ham  pity was in mood for that so any other suggest...\n",
       "5572   ham  the guy did some bitching but i acted like i d...\n",
       "5573   ham                          rofl its true to its name\n",
       "\n",
       "[5574 rows x 2 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel('C:/Users/shubh/SMS.xlsx', header = None)\n",
    "df.columns = ['A', 'B']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ham     4827\n",
      "spam     747\n",
      "Name: A, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# to output the number of spam messages in the dataset\n",
    "count = df['A'].value_counts()\n",
    "print(count)\n",
    "count_spam = count[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 747 number of spam messages in the data set\n"
     ]
    }
   ],
   "source": [
    "print(\"There are\",count_spam,\"number of spam messages in the data set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5574\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# code to find the top 5 frequently occurring words in the messages column of the SMS.txt file.\n",
    "from collections import Counter\n",
    "import xlrd\n",
    "wb = xlrd.open_workbook('SMS.xlsx') \n",
    "sheet = wb.sheet_by_index(0)\n",
    "n_rows = sheet.nrows #Number of Rows\n",
    "print(n_rows)\n",
    "n_cols = sheet.ncols #Number of Columns\n",
    "print(n_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_data = \"\"\n",
    "for current_row in range(0, n_rows, 1):\n",
    "    data = sheet.cell_value(current_row, 1)\n",
    "    #print(data)\n",
    "    result_data = result_data + \" \" + data\n",
    "#print(result_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_data = result_data.split()\n",
    "#print(split_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 5 frequent occurring words are: - \n",
      "[('i', 3032), ('to', 2253), ('you', 2247), ('a', 1466), ('the', 1343)]\n"
     ]
    }
   ],
   "source": [
    "Counter = Counter(split_data)\n",
    "top_5_occur = Counter.most_common(5)\n",
    "print(\"top 5 frequent occurring words are: - \")\n",
    "print(top_5_occur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>on</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>very</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>out</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>most</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>him</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>an</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>doing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>are</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>before</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>153 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0\n",
       "0         i\n",
       "1        on\n",
       "2      very\n",
       "3       out\n",
       "4      most\n",
       "..      ...\n",
       "148     him\n",
       "149      an\n",
       "150   doing\n",
       "151     are\n",
       "152  before\n",
       "\n",
       "[153 rows x 1 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code to find the top 5 most frequent words in the messages column of the SMS.txt file after removing the stopwords shared with us. \n",
    "df1 = pd.read_csv(\"stopwords.txt\", header = None)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'on', 'very', 'out', 'most', 'ourselves', 'above', 'shouldn', 'whom', 'where', 'only', 'can', 'she', 'of', 'further', 'himself', 'd', 'he', 'themselves', 'because', 'the', 'but', 'then', 'having', 'their', 'some', 'we', 'after', 'these', 'ours', 'each', 'should', 'at', 'in', 'off', 'while', 'were', 'our', 'o', 'they', 'doesn', 'haven', 'to', 'between', 'here', 'when', 'nor', 'aren', 'more', 'or', 'y', 'too', 'being', 'no', 'this', 'her', 'mustn', 'same', 'any', 'by', 'was', 'if', 'which', 'during', 'do', 'hadn', 'as', 've', 'had', 'myself', 'his', 'such', 'll', 'your', 'does', 'yourselves', 'you', 'from', 'weren', 'under', 'me', 'yourself', 'yours', 'than', 'with', 'didn', 'couldn', 'them', 'now', 'about', 'have', 'has', 'down', 'be', 'am', 'through', 'my', 'been', 'will', 'wouldn', 'ma', 'those', 'that', 'a', 'so', 'herself', 'not', 'needn', 'against', 's', 'did', 'why', 'for', 'other', 'both', 'won', 'wasn', 'own', 'its', 'shan', 'just', 'it', 'and', 'until', 'is', 'isn', 'once', 'over', 'all', 'mightn', 'how', 'again', 't', 'there', 'into', 'who', 'theirs', 'hers', 're', 'hasn', 'ain', 'few', 'm', 'what', 'below', 'don', 'itself', 'up', 'him', 'an', 'doing', 'are', 'before']\n",
      "153\n"
     ]
    }
   ],
   "source": [
    "#importing the stopwords file and converting it to a dataframe. \n",
    "stop_words = df1[0].tolist()\n",
    "print(stop_words)\n",
    "print(len(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50503\n"
     ]
    }
   ],
   "source": [
    "#removing stopwords from the messages column\n",
    "cleaned_dataset = []\n",
    "for x in split_data:\n",
    "    if x not in stop_words:\n",
    "        cleaned_dataset.append(x)\n",
    "#print(cleaned_dataset)\n",
    "print(len(cleaned_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('u', 1228), ('call', 608), ('get', 397), ('ur', 391), ('gt', 318)]\n"
     ]
    }
   ],
   "source": [
    "# printing the top 5 frequent words after removing the stopwords. \n",
    "from collections import Counter\n",
    "Counter1 = Counter(cleaned_dataset)\n",
    "most_occur = Counter1.most_common(5) \n",
    "print(most_occur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 3 (b) - Writing the messages without the stopwords into a new excel file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5574\n",
      "[['go', 'until', 'jurong', 'point', 'crazy', 'available', 'only', 'in', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet', 'cine', 'there', 'got', 'amore', 'wat'], ['ok', 'lar', 'joking', 'wif', 'u', 'oni'], ['free', 'entry', 'in', 'a', 'wkly', 'comp', 'to', 'win', 'fa', 'cup', 'final', 'tkts', 'st', 'may', 'text', 'fa', 'to', 'to', 'receive', 'entry', 'question', 'std', 'txt', 'rate', 't', 'c', 's', 'apply', 'over', 's'], ['u', 'dun', 'say', 'so', 'early', 'hor', 'u', 'c', 'already', 'then', 'say'], ['nah', 'i', 'don', 't', 'think', 'he', 'goes', 'to', 'usf', 'he', 'lives', 'around', 'here', 'though'], ['freemsg', 'hey', 'there', 'darling', 'it', 's', 'been', 'week', 's', 'now', 'and', 'no', 'word', 'back', 'i', 'd', 'like', 'some', 'fun', 'you', 'up', 'for', 'it', 'still', 'tb', 'ok', 'xxx', 'std', 'chgs', 'to', 'send', 'to', 'rcv'], ['even', 'my', 'brother', 'is', 'not', 'like', 'to', 'speak', 'with', 'me', 'they', 'treat', 'me', 'like', 'aids', 'patent'], ['as', 'per', 'your', 'request', 'melle', 'melle', 'oru', 'minnaminunginte', 'nurungu', 'vettam', 'has', 'been', 'set', 'as', 'your', 'callertune', 'for', 'all', 'callers', 'press', 'to', 'copy', 'your', 'friends', 'callertune'], ['winner', 'as', 'a', 'valued', 'network', 'customer', 'you', 'have', 'been', 'selected', 'to', 'receivea', 'prize', 'reward', 'to', 'claim', 'call', 'claim', 'code', 'kl', 'valid', 'hours', 'only'], ['had', 'your', 'mobile', 'months', 'or', 'more', 'u', 'r', 'entitled', 'to', 'update', 'to', 'the', 'latest', 'colour', 'mobiles', 'with', 'camera', 'for', 'free', 'call', 'the', 'mobile', 'update', 'co', 'free', 'on']]\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "wb1 = xlrd.open_workbook('SMS.xlsx') \n",
    "sheet1 = wb1.sheet_by_index(0)\n",
    "num_rows = sheet1.nrows \n",
    "print(num_rows)\n",
    "msg_list = []\n",
    "for ele in range(0, num_rows, 1):\n",
    "    data1 = sheet1.cell_value(ele, 1)\n",
    "    list_of_words = data1.split(\" \")\n",
    "    #print(data1)\n",
    "    msg_list.append(list_of_words)\n",
    "print(msg_list[:10])\n",
    "print(type(msg_list[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords and simultaneously write the messages into a new file\n",
    "import openpyxl\n",
    "wbout = openpyxl.Workbook() \n",
    "#wbout.create_sheet(index = 0 , title = \"sheet1\")\n",
    "wsout = wbout.active\n",
    "initial_counter_val = 1\n",
    "\n",
    "for msg in msg_list:\n",
    "    new_msg_list = \"\"\n",
    "    for word in msg:\n",
    "        if word not in stop_words:\n",
    "            new_msg_list = new_msg_list + word + \" \"\n",
    "    #print(\"pre-processed message with no stop-words \",new_msg_list)\n",
    "    (wsout.cell(initial_counter_val, column = 1)).value = new_msg_list\n",
    "    initial_counter_val+=1\n",
    "wbout.save(\"C:/Users/shubh/op.xlsx\")  #directory for the python files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5574\n",
      "['go jurong point crazy available bugis n great world la e buffet cine got amore wat ', 'ok lar joking wif u oni ', 'free entry wkly comp win fa cup final tkts st may text fa receive entry question std txt rate c apply ', 'u dun say early hor u c already say ', 'nah think goes usf lives around though ', 'freemsg hey darling week word back like fun still tb ok xxx std chgs send rcv ', 'even brother like speak treat like aids patent ', 'per request melle melle oru minnaminunginte nurungu vettam set callertune callers press copy friends callertune ', 'winner valued network customer selected receivea prize reward claim call claim code kl valid hours ', 'mobile months u r entitled update latest colour mobiles camera free call mobile update co free ']\n",
      "<class 'list'>\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "7624\n",
      "5574\n"
     ]
    }
   ],
   "source": [
    "# PART 4a\n",
    "# converting the messages to vectors using countvectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import sys\n",
    "import numpy\n",
    "numpy.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "df2 = pd.read_excel(\"op.xlsx\", header = None)\n",
    "df2.head(5)\n",
    "vectorizer = CountVectorizer()\n",
    "# convert the op file into a list of strings\n",
    "wb1 = xlrd.open_workbook('op.xlsx') #reinitializing some global variables\n",
    "sheet1 = wb1.sheet_by_index(0)\n",
    "num_rows = sheet1.nrows \n",
    "print(num_rows)\n",
    "msg_list = []\n",
    "for ele in range(0, num_rows, 1):\n",
    "    data1 = sheet1.cell_value(ele, 0)\n",
    "    #print(data1)\n",
    "    msg_list.append(data1)\n",
    "print(msg_list[:10])\n",
    "print(type(msg_list))\n",
    "\n",
    "\n",
    "X = vectorizer.fit_transform(msg_list)\n",
    "print(type(X))\n",
    "#print(X)\n",
    "#print(vectorizer.get_feature_names())\n",
    "#print(vectorizer.vocabulary_)\n",
    "print(len(vectorizer.vocabulary_))\n",
    "#print(X.toarray())\n",
    "Y = X.toarray()\n",
    "print(len(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0668\n",
      "0.06681531047810607\n"
     ]
    }
   ],
   "source": [
    "# PART 4b\n",
    "from scipy import spatial\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# calculating the Cosine Similarity between message 1 and 50. \n",
    "v1 = Y[0] #considering that message 1 is at location 0 of array Y and message 50 is at location 49 \n",
    "v2 = Y[49]\n",
    "\n",
    "dot = numpy.dot(v1,v2)\n",
    "normv1 = numpy.linalg.norm(v1)\n",
    "normv2 = numpy.linalg.norm(v2)\n",
    "cos_sim = dot / (normv1 * normv2)\n",
    "print(round(cos_sim,4))\n",
    "\n",
    "#code to verify the result\n",
    "cos_similarity = 1 - spatial.distance.cosine(v1, v2)\n",
    "print(cos_similarity)\n",
    "#print(cosine_similarity(v1,v2))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
